{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# reload custom modules imported during runtime\n",
        "# https://stackoverflow.com/questions/50339549/google-colab-reload-imported-modules\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "uEt7mctkijNS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium['all']\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n"
      ],
      "metadata": {
        "id": "lszDJ1jOVDGm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define environment\n",
        "# N-back environment\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import Env, spaces, utils\n",
        "\n",
        "class NBack(Env):\n",
        "\n",
        "    # Examples\n",
        "    # N = 2\n",
        "    # step_count =        [ 0  1   2  3  4  5  6 ]\n",
        "    # sequence =          [ a  b   c  d  a  d  a ] (except these are usually digits between 0-9)\n",
        "    # correct actions =   [ ~  ~   0  0  0  1  1 ]\n",
        "    # actions =           [ ~  ~   1  0  0  1  0 ]\n",
        "    # reward_class =      [ ~  ~  FP TN TN TP FN ]\n",
        "    # reward =            [ ~  ~  -1  0  0  1 -1 ]\n",
        "    # Rewards input is structured as (TP, TN, FP, FN) (positive being matches)\n",
        "\n",
        "    def __init__(self, N=2, num_trials=25, num_targets=None, rewards=(1, 1, -1, -1), num_obs=5, seed=2023):\n",
        "\n",
        "        self.N = N\n",
        "        self.num_trials = num_trials\n",
        "        self.episode_length = num_trials + self.N\n",
        "        self.num_targets = num_targets\n",
        "        self.rewards = rewards\n",
        "        self.num_obs = num_obs\n",
        "        self.num_actions = 2\n",
        "        # super().reset(seed=seed)\n",
        "\n",
        "        # Check that parameters are legal\n",
        "        assert(len(rewards) == 4)\n",
        "        assert(num_targets is None or num_targets <= num_trials)\n",
        "\n",
        "        # Define rewards, observation space and action space\n",
        "        self.reward_range = (min(rewards), max(rewards))    # Range of rewards based on inputs\n",
        "        # self.observation_space = spaces.Tuple([spaces.Discrete(10) for i in range(self.num_obs)])     # Tuple num_obs long with 10 possibilities\n",
        "        self.observation_space = spaces.Box(low=0, high=9, shape=(5, ))\n",
        "        self.action_space = spaces.Discrete(self.num_actions)                        # 0 (No match) or 1 (Match)\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "\n",
        "        # Seed RNG\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Generate sequence and correct actions\n",
        "        self._generate_sequence()\n",
        "        self._get_correct_actions()\n",
        "\n",
        "        # Observation is first character\n",
        "        self.step_count = 0\n",
        "\n",
        "        # initialize\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        # Calculate reward\n",
        "        if self.step_count >= self.N:\n",
        "            if (self.correct_actions[self.step_count - self.N]): # Match\n",
        "                reward = self.rewards[0] if action else self.rewards[3] # TP if matched else FN\n",
        "            else: # No match\n",
        "                reward = self.rewards[2] if action else self.rewards[1] # FP if matches else TN\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        # Return next character or None\n",
        "\n",
        "        self.step_count += 1\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.step_count < self.episode_length:\n",
        "            return observation, reward, False, info\n",
        "        else:\n",
        "            return observation, reward, True, info\n",
        "\n",
        "    def _generate_sequence(self):\n",
        "\n",
        "        # Generate sequence of length self.episode_length (with correct number of targets)\n",
        "        while True:\n",
        "            self.sequence = np.random.randint(0, 9, size=(self.episode_length))\n",
        "            if not self.num_targets or sum(self._get_correct_actions()) == self.num_targets:\n",
        "                break\n",
        "\n",
        "\n",
        "    def _get_obs(self):\n",
        "\n",
        "        if self.step_count < self.num_obs:\n",
        "            window = self.sequence[:self.step_count + 1]\n",
        "            observation = np.pad(window, (self.num_obs - self.step_count -1, 0), mode='constant', constant_values=(0))\n",
        "        elif self.step_count == self.episode_length:\n",
        "            window = self.sequence[self.step_count + 1 - self.num_obs : self.step_count + 1]\n",
        "            observation = np.pad(window, (0,1), mode='constant', constant_values=(0))\n",
        "        else:\n",
        "            window = self.sequence[self.step_count + 1 - self.num_obs : self.step_count + 1]\n",
        "            observation = window\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def _get_correct_actions(self):\n",
        "        self.correct_actions = np.array([int(self.sequence[i] == self.sequence[i + self.N]) for i in range(self.num_trials)])\n",
        "        return self.correct_actions\n",
        "\n",
        "    def _get_info(self):\n",
        "        info = {\n",
        "            'step_count': self.step_count,\n",
        "            }\n",
        "        return info\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "tIDtUzTVP1GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import register\n",
        "\n",
        "register(\n",
        "    id='NBack-v0',\n",
        "    entry_point='nback_env:NBack',\n",
        ")\n",
        "\n",
        "env = gym.make(\n",
        "    'NBack-v0',\n",
        "    N = 3,\n",
        "    num_trials=100,\n",
        "    num_targets=10,\n",
        "    rewards=(1, 0, 0, 0),\n",
        "    num_obs=5,\n",
        "    seed=2023\n",
        "    )\n",
        "\n",
        "observation, info = env.reset()\n",
        "print(f\"reset observation:\\t{observation}\\n\")\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    next_observation, reward, done, info = env.step(action)\n",
        "    print(f\"observation:\\t{observation}\")\n",
        "    print(f\"action:\\t{action}\")\n",
        "    print(f\"reward:\\t{reward}\")\n",
        "    print(f\"next_observation:\\t{next_observation}\")\n",
        "    print(f\"done:\\t{done}\")\n",
        "    print(f\"info:\\t{info['step_count']}\")\n",
        "    print(f\"\\n\")\n",
        "    observation = next_observation"
      ],
      "metadata": {
        "id": "_xSgz1N1VdJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, next_state, reward, done):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, next_state, reward, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, next_states, rewards, dones = zip(*batch)\n",
        "        return np.stack(states), actions, np.stack(next_states), rewards, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ],
      "metadata": {
        "id": "w16zUF4MVGEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234aaf6d-366b-42fc-f651-dc1af259f2e6"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[autoreload of nback_env failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
            "    update_generic(old_obj, new_obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
            "    if update_generic(old_obj, new_obj): continue\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
            "    setattr(old, name, getattr(new, name))\n",
            "ValueError: __init__() requires a code object with 1 free vars, not 0\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, memory, optimizer, criterion, batch_size, gamma):\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "    states, actions, next_states, rewards, dones = memory.sample(batch_size)\n",
        "    states = Variable(torch.FloatTensor(states))\n",
        "    actions = Variable(torch.LongTensor(actions))\n",
        "    next_states = Variable(torch.FloatTensor(next_states))\n",
        "    rewards = Variable(torch.FloatTensor(rewards))\n",
        "    dones = Variable(torch.FloatTensor(dones))\n",
        "\n",
        "    q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "    next_q_values = model(next_states).max(1)[0]\n",
        "\n",
        "    target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "    loss = criterion(q_values, target_q_values.detach())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def exec_training(env, gamma=0.99, num_episodes = 100):\n",
        "\n",
        "    # Create an instance of the DQN model\n",
        "    input_size = env.observation_space.shape[0]\n",
        "    output_size = env.action_space.n\n",
        "    model = DQN(input_size, output_size)\n",
        "\n",
        "    # Create an instance of the replay memory\n",
        "    capacity = 1000\n",
        "    memory = ReplayMemory(capacity)\n",
        "\n",
        "    # Set hyperparameters\n",
        "    batch_size = 64\n",
        "    lr = 0.001\n",
        "    # gamma = 0.99\n",
        "    # num_episodes = 100\n",
        "\n",
        "    # Set up the optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    rewards_list = []\n",
        "    # Training loop\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        state = state[0]\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        step_count = 1\n",
        "        while not done:\n",
        "            # Select an action using epsilon-greedy policy\n",
        "            epsilon = max(0.01, 0.08 - 0.01 * episode)\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = model(torch.FloatTensor(state))\n",
        "                    action = q_values.argmax().item()\n",
        "\n",
        "            # Take the selected action and observe the next state and reward\n",
        "            # next_state, reward, done, terminated, truncated = env.step(action)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Store the transition in the replay memory\n",
        "            memory.push(state, action, next_state, reward, done)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            # Train the model\n",
        "            train(model, memory, optimizer, criterion, batch_size, gamma)\n",
        "            step_count += 1\n",
        "\n",
        "        rewards_list.append(total_reward)\n",
        "\n",
        "        # Print the total reward for the episode\n",
        "        # print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "    return rewards_list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jwT5leMcWQen"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment, random agent and test\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import register\n",
        "\n",
        "register(\n",
        "    id='NBack-v0',\n",
        "    entry_point='nback_env:NBack',\n",
        ")\n",
        "\n",
        "env = gym.make(\n",
        "    'NBack-v0',\n",
        "    N = 2,\n",
        "    num_trials=100,\n",
        "    num_targets=1,\n",
        "    rewards=(1, 0, 0, 0),\n",
        "    num_obs=,\n",
        "    seed=42\n",
        "    )\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "re1 = exec_training(env=env, gamma=0.0, num_episodes = 50)\n",
        "re2 = exec_training(env=env, gamma=0.5, num_episodes = 50)\n",
        "re3 = exec_training(env=env, gamma=0.99, num_episodes = 50)\n"
      ],
      "metadata": {
        "id": "CCJoSD7CSLcg"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import ReplicationPad3d\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Create the data sequences\n",
        "\n",
        "y1 = np.array(re1)  # Example y values for line 1\n",
        "y2 = np.array(re2)  # Example y values for line 1\n",
        "y3 = np.array(re3)  # Example y values for line 1\n",
        "x = np.arange(y1.shape[0])  # Example x values\n",
        "\n",
        "# Create the line traces\n",
        "trace1 = go.Scatter(x=x, y=y1, mode='lines', name='Line 1')\n",
        "trace2 = go.Scatter(x=x, y=y2, mode='lines', name='Line 2')\n",
        "trace3 = go.Scatter(x=x, y=y3, mode='lines', name='Line 3')\n",
        "\n",
        "# Create the layout\n",
        "layout = go.Layout(\n",
        "    title='Line Plot with Three Lines',\n",
        "    xaxis=dict(title='X-axis'),\n",
        "    yaxis=dict(title='Y-axis')\n",
        ")\n",
        "\n",
        "# Combine the traces and layout\n",
        "data = [\n",
        "    trace1,\n",
        "    trace2,\n",
        "    trace3\n",
        "    ]\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "HR8qR36Af9Sq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "8b56cb38-8fe1-4195-b5cf-dcab434b41cc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"8ebc8540-7317-47a5-b584-220d4474403e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8ebc8540-7317-47a5-b584-220d4474403e\")) {                    Plotly.newPlot(                        \"8ebc8540-7317-47a5-b584-220d4474403e\",                        [{\"mode\":\"lines\",\"name\":\"Line 1\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[6,8,10,8,13,16,8,8,13,11,8,7,8,10,6,13,10,11,13,10,10,13,12,16,11,10,14,9,8,16,13,8,11,12,11,8,12,11,7,14,8,8,9,10,11,13,13,11,12,5],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Line 2\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0,4,13,15,10,8,7,7,8,9,13,9,17,14,13,16,12,12,7,9,11,5,14,12,8,10,18,12,9,11,12,13,17,7,7,7,15,12,12,8,11,8,12,16,10,15,11,8,11,7],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Line 3\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[2,10,7,8,4,6,7,3,6,3,5,9,10,10,5,15,13,7,11,8,8,4,6,8,7,5,6,7,5,10,6,9,5,4,9,13,8,5,13,12,4,7,8,13,9,13,13,7,13,9],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Line Plot with Three Lines\"},\"xaxis\":{\"title\":{\"text\":\"X-axis\"}},\"yaxis\":{\"title\":{\"text\":\"Y-axis\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8ebc8540-7317-47a5-b584-220d4474403e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GqsEmvi_G7o7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}