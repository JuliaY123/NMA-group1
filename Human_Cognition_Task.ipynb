{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @markdown  ### reload custom modules imported during runtime\n",
        "# https://stackoverflow.com/questions/50339549/google-colab-reload-imported-modules\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "uEt7mctkijNS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Load custom environment as module into content folder.\n",
        "\n",
        "import os\n",
        "if not os.path.exists('NMA-group1'):\n",
        "    !git clone -b impl_surya https://github.com/JuliaY123/NMA-group1.git\n",
        "!cp '/content/NMA-group1/nback_env.py' '/content/nback_env.py'\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n"
      ],
      "metadata": {
        "id": "woogehqDEKOc",
        "outputId": "087a60b1-b476-413b-c8f7-b7fddd09a381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NMA-group1'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 134 (delta 32), reused 31 (delta 28), pack-reused 96\u001b[K\n",
            "Receiving objects: 100% (134/134), 1.66 MiB | 3.70 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Supress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "4_Xbz7T4maZL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "!pip install swig\n",
        "!pip install gymnasium['all']\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n"
      ],
      "metadata": {
        "id": "lszDJ1jOVDGm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define environment\n",
        "# # N-back environment\n",
        "\n",
        "# import numpy as np\n",
        "# import gymnasium as gym\n",
        "# from gymnasium import Env, spaces, utils\n",
        "\n",
        "# class NBack(Env):\n",
        "\n",
        "#     # Examples\n",
        "#     # N = 2\n",
        "#     # step_count =        [ 0  1   2  3  4  5  6 ]\n",
        "#     # sequence =          [ a  b   c  d  a  d  a ] (except these are usually digits between 0-9)\n",
        "#     # correct actions =   [ ~  ~   0  0  0  1  1 ]\n",
        "#     # actions =           [ ~  ~   1  0  0  1  0 ]\n",
        "#     # reward_class =      [ ~  ~  FP TN TN TP FN ]\n",
        "#     # reward =            [ ~  ~  -1  0  0  1 -1 ]\n",
        "#     # Rewards input is structured as (TP, TN, FP, FN) (positive being matches)\n",
        "\n",
        "#     def __init__(self, N=2, num_trials=25, num_targets=None, rewards=(1, 1, -1, -1), num_obs=5, seed=2023):\n",
        "\n",
        "#         self.N = N\n",
        "#         self.num_trials = num_trials\n",
        "#         self.episode_length = num_trials + self.N\n",
        "#         self.num_targets = num_targets\n",
        "#         self.rewards = rewards\n",
        "#         self.num_obs = num_obs\n",
        "#         self.num_actions = 2\n",
        "#         # super().reset(seed=seed)\n",
        "\n",
        "#         # Check that parameters are legal\n",
        "#         assert(len(rewards) == 4)\n",
        "#         assert(num_targets is None or num_targets <= num_trials)\n",
        "\n",
        "#         # Define rewards, observation space and action space\n",
        "#         self.reward_range = (min(rewards), max(rewards))    # Range of rewards based on inputs\n",
        "#         # self.observation_space = spaces.Tuple([spaces.Discrete(10) for i in range(self.num_obs)])     # Tuple num_obs long with 10 possibilities\n",
        "#         self.observation_space = spaces.Box(low=0, high=9, shape=(5, ))\n",
        "#         self.action_space = spaces.Discrete(self.num_actions)                        # 0 (No match) or 1 (Match)\n",
        "\n",
        "#     def reset(self, seed=None):\n",
        "\n",
        "#         # Seed RNG\n",
        "#         super().reset(seed=seed)\n",
        "\n",
        "#         # Generate sequence and correct actions\n",
        "#         self._generate_sequence()\n",
        "#         self._get_correct_actions()\n",
        "\n",
        "#         # Observation is first character\n",
        "#         self.step_count = 0\n",
        "\n",
        "#         # initialize\n",
        "#         observation = self._get_obs()\n",
        "#         info = self._get_info()\n",
        "\n",
        "#         return observation, info\n",
        "\n",
        "#     def step(self, action):\n",
        "\n",
        "#         # Calculate reward\n",
        "#         if self.step_count >= self.N:\n",
        "#             if (self.correct_actions[self.step_count - self.N]): # Match\n",
        "#                 reward = self.rewards[0] if action else self.rewards[3] # TP if matched else FN\n",
        "#             else: # No match\n",
        "#                 reward = self.rewards[2] if action else self.rewards[1] # FP if matches else TN\n",
        "#         else:\n",
        "#             reward = 0\n",
        "\n",
        "#         # Return next character or None\n",
        "\n",
        "#         self.step_count += 1\n",
        "#         observation = self._get_obs()\n",
        "#         info = self._get_info()\n",
        "\n",
        "#         if self.step_count < self.episode_length:\n",
        "#             return observation, reward, False, info\n",
        "#         else:\n",
        "#             return observation, reward, True, info\n",
        "\n",
        "#     def _generate_sequence(self):\n",
        "\n",
        "#         # Generate sequence of length self.episode_length (with correct number of targets)\n",
        "#         while True:\n",
        "#             self.sequence = np.random.randint(0, 9, size=(self.episode_length))\n",
        "#             if not self.num_targets or sum(self._get_correct_actions()) == self.num_targets:\n",
        "#                 break\n",
        "\n",
        "\n",
        "#     def _get_obs(self):\n",
        "\n",
        "#         if self.step_count < self.num_obs:\n",
        "#             window = self.sequence[:self.step_count + 1]\n",
        "#             observation = np.pad(window, (self.num_obs - self.step_count -1, 0), mode='constant', constant_values=(0))\n",
        "#         elif self.step_count == self.episode_length:\n",
        "#             window = self.sequence[self.step_count + 1 - self.num_obs : self.step_count + 1]\n",
        "#             observation = np.pad(window, (0,1), mode='constant', constant_values=(0))\n",
        "#         else:\n",
        "#             window = self.sequence[self.step_count + 1 - self.num_obs : self.step_count + 1]\n",
        "#             observation = window\n",
        "\n",
        "#         return observation\n",
        "\n",
        "#     def _get_correct_actions(self):\n",
        "#         self.correct_actions = np.array([int(self.sequence[i] == self.sequence[i + self.N]) for i in range(self.num_trials)])\n",
        "#         return self.correct_actions\n",
        "\n",
        "#     def _get_info(self):\n",
        "#         info = {\n",
        "#             'step_count': self.step_count,\n",
        "#             }\n",
        "#         return info\n",
        "\n"
      ],
      "metadata": {
        "id": "tIDtUzTVP1GV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Random Agent\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import register\n",
        "\n",
        "register(\n",
        "    id='NBack-v0',\n",
        "    entry_point='nback_env:NBack',\n",
        ")\n",
        "\n",
        "env = gym.make(\n",
        "    'NBack-v0',\n",
        "    N = 3,\n",
        "    num_trials=100,\n",
        "    num_targets=10,\n",
        "    rewards=(1, 0, 0, 0),\n",
        "    num_obs=5,\n",
        "    seed=2023\n",
        "    )\n",
        "\n",
        "observation, info = env.reset()\n",
        "print(f\"reset observation:\\t{observation}\\n\")\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    next_observation, reward, done, info = env.step(action)\n",
        "    print(f\"observation:\\t{observation}\")\n",
        "    print(f\"action:\\t{action}\")\n",
        "    print(f\"reward:\\t{reward}\")\n",
        "    print(f\"next_observation:\\t{next_observation}\")\n",
        "    print(f\"done:\\t{done}\")\n",
        "    print(f\"info:\\t{info['step_count']}\")\n",
        "    print(f\"\\n\")\n",
        "    observation = next_observation"
      ],
      "metadata": {
        "id": "_xSgz1N1VdJd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DQN Agent with Experience Replay\n",
        "\n",
        "import random\n",
        "\n",
        "class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, next_state, reward, done):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, next_state, reward, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, next_states, rewards, dones = zip(*batch)\n",
        "        return np.stack(states), actions, np.stack(next_states), rewards, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.softmax(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "w16zUF4MVGEe"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DQN with Recurrent Networks and Experience Replay\n",
        "\n",
        "import random\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, next_state, reward, done):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, next_state, reward, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, next_states, rewards, dones = zip(*batch)\n",
        "        return np.stack(states), actions, np.stack(next_states), rewards, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class DRQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DRQN, self).__init__()\n",
        "        self.rnn1 = nn.RNN(input_size, 64, 2, batch_first=True)\n",
        "        self.fc1 = nn.Linear(64, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r_out, hidden = self.rnn1(x)\n",
        "        r_out = r_out.view(-1, 64)\n",
        "        x = self.relu(self.rnn1(x))\n",
        "        x = self.softmax(self.fc1(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AMNKScbknpI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, memory, optimizer, criterion, batch_size, gamma):\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "    states, actions, next_states, rewards, dones = memory.sample(batch_size)\n",
        "    states = Variable(torch.FloatTensor(states))\n",
        "    actions = Variable(torch.LongTensor(actions))\n",
        "    next_states = Variable(torch.FloatTensor(next_states))\n",
        "    rewards = Variable(torch.FloatTensor(rewards))\n",
        "    dones = Variable(torch.FloatTensor(dones))\n",
        "\n",
        "    q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "    next_q_values = model(next_states).max(1)[0]\n",
        "\n",
        "    target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "    loss = criterion(q_values, target_q_values.detach())\n",
        "    loss_value = loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss_value\n",
        "\n",
        "\n",
        "def exec_training(\n",
        "    env,\n",
        "    gamma=0.99,\n",
        "    num_episodes = 100,\n",
        "    capacity = 1000,\n",
        "    batch_size = 64,\n",
        "    lr = 0.001\n",
        "    ):\n",
        "\n",
        "    # Create an instance of the DQN model\n",
        "    input_size = env.observation_space.shape[0]\n",
        "    output_size = env.action_space.n\n",
        "    model = DQN(input_size, output_size)\n",
        "\n",
        "    # Create an instance of the replay memory\n",
        "    memory = ReplayMemory(capacity)\n",
        "\n",
        "    # Set up the optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    rewards_list = []\n",
        "    loss_list = []\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        state = state[0]\n",
        "        done = False\n",
        "\n",
        "        total_reward = 0\n",
        "        episode_loss = []\n",
        "        step_count = 1\n",
        "\n",
        "        while not done:\n",
        "            # Select an action using epsilon-greedy policy\n",
        "            epsilon = max(0.01, 0.08 - 0.01 * episode)\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = model(torch.FloatTensor(state))\n",
        "                    action = q_values.argmax().item()\n",
        "\n",
        "            # Take the selected action and observe the next state and reward\n",
        "            # next_state, reward, done, terminated, truncated = env.step(action)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Store the transition in the replay memory\n",
        "            memory.push(state, action, next_state, reward, done)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            # Train the model\n",
        "            loss = train(model, memory, optimizer, criterion, batch_size, gamma)\n",
        "\n",
        "            if loss is None:\n",
        "                loss = 0\n",
        "            episode_loss.append(loss)\n",
        "\n",
        "            step_count += 1\n",
        "\n",
        "        rewards_list.append(total_reward)\n",
        "        loss_list.append(np.mean(episode_loss))\n",
        "\n",
        "        # Print the total reward for the episode\n",
        "        # print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "    return rewards_list, loss_list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jwT5leMcWQen"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment, random agent and test\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import register\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "register(\n",
        "    id='NBack-v0',\n",
        "    entry_point='nback_env:NBack',\n",
        ")\n",
        "\n",
        "env = gym.make(\n",
        "    'NBack-v0',\n",
        "    N = 2,\n",
        "    num_trials=100,\n",
        "    num_targets=20,\n",
        "    rewards=(1, 0, -1, -1),\n",
        "    num_obs=5,\n",
        "    seed=SEED\n",
        "    )\n",
        "\n",
        "observation, info = env.reset()\n",
        "rewards, losses = exec_training(env=env, gamma=0.99, num_episodes = 1)"
      ],
      "metadata": {
        "id": "CCJoSD7CSLcg",
        "outputId": "b537b05f-0224-40cc-b651-dee30f5ea885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n",
            "0 0\n",
            "1 0\n",
            "2 0\n",
            "3 1\n",
            "4 1\n",
            "5 0\n",
            "6 0\n",
            "7 0\n",
            "8 0\n",
            "9 1\n",
            "10 0\n",
            "11 1\n",
            "12 0\n",
            "13 1\n",
            "14 0\n",
            "15 0\n",
            "16 0\n",
            "17 0\n",
            "18 0\n",
            "19 0\n",
            "20 0\n",
            "21 0\n",
            "22 0\n",
            "23 0\n",
            "24 0\n",
            "25 0\n",
            "26 0\n",
            "27 0\n",
            "28 0\n",
            "29 0\n",
            "30 0\n",
            "31 1\n",
            "32 0\n",
            "33 1\n",
            "34 0\n",
            "35 1\n",
            "36 0\n",
            "37 0\n",
            "38 0\n",
            "39 0\n",
            "40 1\n",
            "41 0\n",
            "42 0\n",
            "43 0\n",
            "44 0\n",
            "45 0\n",
            "46 0\n",
            "47 0\n",
            "48 0\n",
            "49 0\n",
            "50 0\n",
            "51 0\n",
            "52 1\n",
            "53 0\n",
            "54 0\n",
            "55 0\n",
            "56 0\n",
            "57 0\n",
            "58 0\n",
            "59 0\n",
            "60 1\n",
            "61 0\n",
            "62 0\n",
            "63 0\n",
            "64 0\n",
            "65 1\n",
            "66 0\n",
            "67 1\n",
            "68 0\n",
            "69 0\n",
            "70 0\n",
            "71 1\n",
            "72 0\n",
            "73 1\n",
            "74 0\n",
            "75 1\n",
            "76 1\n",
            "77 0\n",
            "78 0\n",
            "79 0\n",
            "80 0\n",
            "81 0\n",
            "82 0\n",
            "83 0\n",
            "84 0\n",
            "85 0\n",
            "86 1\n",
            "87 1\n",
            "88 0\n",
            "89 0\n",
            "90 0\n",
            "91 0\n",
            "92 0\n",
            "93 1\n",
            "94 0\n",
            "95 0\n",
            "96 0\n",
            "97 0\n",
            "98 0\n",
            "99 0\n",
            "100 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-df141320d123>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexec_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-d51460a2ecc8>\u001b[0m in \u001b[0;36mexec_training\u001b[0;34m(env, gamma, num_episodes, capacity, batch_size, lr)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Take the selected action and observe the next state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# next_state, reward, done, terminated, truncated = env.step(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# Store the transition in the replay memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/nback_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/nback_env.py\u001b[0m in \u001b[0;36m_get_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         info = {\n\u001b[1;32m    110\u001b[0m             \u001b[0;34m'step_count'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 0 with size 100"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import itertools\n",
        "\n",
        "fig = make_subplots(rows=2, cols=1)\n",
        "# fig = go.Figure()\n",
        "\n",
        "def generate_figure_traces(data_dict):\n",
        "\n",
        "    y = data_dict['data']\n",
        "    label = data_dict['label']\n",
        "    # Create the data sequence\n",
        "    y = np.array(y)  # Example y values for line 1\n",
        "    x = np.arange(y.shape[0])  # Example x values\n",
        "\n",
        "    # Create the line trace\n",
        "    trace = go.Scatter(x=x, y=y, mode='lines', name=label)\n",
        "\n",
        "    return trace\n",
        "\n",
        "data = [\n",
        "    {'data': rewards, 'label': 'reward'},\n",
        "    {'data': losses, 'label': 'loss'},\n",
        "    ]\n",
        "\n",
        "traces = list(map(lambda x: generate_figure_traces(x), data))\n",
        "\n",
        "for index, trace in enumerate(traces):\n",
        "    # add the first plot\n",
        "    fig.add_trace(\n",
        "        trace,\n",
        "        row=index+1, col=1\n",
        "    )\n",
        "\n",
        "    # Create the layout\n",
        "    fig.update_layout(\n",
        "        title='Metrics Plot',\n",
        "    )\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "HR8qR36Af9Sq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "0e8229f2-027c-459e-c20d-4037efbe7a9d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"4c7415d1-8f87-4643-a6f5-582ba892440a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4c7415d1-8f87-4643-a6f5-582ba892440a\")) {                    Plotly.newPlot(                        \"4c7415d1-8f87-4643-a6f5-582ba892440a\",                        [{\"mode\":\"lines\",\"name\":\"reward\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],\"y\":[-45,-22,0,-8,11,15,18,18,16,18,20,20,20,20,20,20,20,20,20,19,18,18,19,20,17],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],\"y\":[0.19693654629529692,0.30423468935723397,0.2552354366463773,0.23120475560426712,0.20733706682336098,0.1862870378091055,0.16687806425433532,0.17353464918685893,0.16883142387457922,0.16073165132718928,0.1699849735288059,0.1827312268901105,0.1784999575831142,0.19516955520592483,0.18656717832474148,0.2011240708331267,0.1959871102635767,0.19389019292943618,0.1949499155960831,0.21348312876972497,0.2008458477463208,0.19664415483381234,0.21079151982478067,0.2008087269085295,0.19218382129774375],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.575,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,0.425]},\"title\":{\"text\":\"Metrics Plot\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4c7415d1-8f87-4643-a6f5-582ba892440a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}