{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2ueYfnZ9867"
      },
      "source": [
        "# Investigating RNNs and RL using the N-back cognitive task\n",
        "\n",
        "**NMA 2023 Group 1 Project**\n",
        "\n",
        "__Content creators:__ Aland Astudillo, Campbell Border, Disheng, Julia Yin, Koffivi, Rishabh Mallik, Shuwen Liu, Zelin Zhang\n",
        "\n",
        "__Pod TA:__ Suryanarayanan Nagar Anthel Venkatesh\n",
        "\n",
        "__Project Mentor:__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Objective\n",
        "\n",
        "- \n",
        "\n",
        "- \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwxGrBIy-jz4"
      },
      "source": [
        "# Project Design\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb2cp2_wCSCI"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6eFAloFCZog",
        "outputId": "d2f4791b-85e9-4176-8a30-c205f952f451"
      },
      "outputs": [],
      "source": [
        "# @title Install dependencies\n",
        "!pip install jedi --quiet\n",
        "!pip install --upgrade pip setuptools wheel --quiet\n",
        "!pip install numpy==1.23.3 --quiet --ignore-installed\n",
        "!pip install gymnasium --quiet\n",
        "\n",
        "\n",
        "#!pip install dm-acme[jax] --quiet\n",
        "#!pip install dm-sonnet --quiet\n",
        "#!pip install trfl --quiet\n",
        "#!pip uninstall seaborn -y --quiet\n",
        "#!pip install seaborn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pEza9W8KDK1y"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "#import time\n",
        "import numpy as np\n",
        "#import pandas as pd\n",
        "#import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figure settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Zb0JSXLiECA6"
      },
      "outputs": [],
      "source": [
        "# @title Figure settings\n",
        "from IPython.display import clear_output, display, HTML\n",
        "%matplotlib inline\n",
        "# sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Background\n",
        "\n",
        "## Replace with our own literature review\n",
        "\n",
        "- Cognitive scientists use standard lab tests to tap into specific processes in the brain and behavior. Some examples of those tests are Stroop, N-back, Digit Span, TMT (Trail making tests), and WCST (Wisconsin Card Sorting Tests).\n",
        "\n",
        "- Despite an extensive body of research that explains human performance using descriptive what-models, we still need a more sophisticated approach to gain a better understanding of the underlying processes (i.e., a how-model).\n",
        "\n",
        "- Interestingly, many of such tests can be thought of as a continuous stream of stimuli and corresponding actions, that is in consonant with the RL formulation. In fact, RL itself is in part motivated by how the brain enables goal-directed behaviors using reward systems, making it a good choice to explain human performance.\n",
        "\n",
        "- One behavioral test example would be the N-back task.\n",
        "\n",
        "  - In the N-back, participants view a sequence of stimuli, one by one, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedback is given at both timestep and trajectory levels.\n",
        "\n",
        "  - The agent is rewarded when its response matches the stimulus that was shown N steps back in the episode. A simpler version of the N-back uses two-choice action schema, that is match vs non-match. Once the present stimulus matches the one presented N step back, then the agent is expected to respond to it as being a `match`.\n",
        "\n",
        "\n",
        "- Given a trained RL agent, we then find correlates of its fitted parameters with the brain mechanisms. The most straightforward composition could be the correlation of model parameters with the brain activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets\n",
        "\n",
        "- Human Connectome Project Working Memory (HCP WM) task ([NMA-CN HCP notebooks](https://github.com/NeuromatchAcademy/course-content/tree/master/projects/fMRI))\n",
        "\n",
        "1200 subjects, each subject experience 8 blocks of 2-back and 8 blocks of 0-back.\n",
        "\n",
        "## N-back Tasks\n",
        "\n",
        "In the N-back task, participants view a sequence of stimuli, one per time, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedbacks are given at both timestep and trajectory levels.\n",
        "\n",
        "In a typical neuro setup, both accuracy and response time are measured, but here, for the sake of brevity, we focus only on accuracy of responses.\n",
        "\n",
        "- 2 back working memory task:\n",
        "\n",
        "The second condition is a 2-Back condition. During such a block, the subject is presented with a sequence of 10 images and must respond if each image is identical to the one 2 positions earlier or not (figure, right). At the beginning of the block there is a cue screen informing the subject that the upcoming stimuli are part of the 2-Back protocol. The timing of the cue screen, the presentation of the 10 stimulus images and of the response interval are identical to that of the 0-Back condition.\n",
        "\n",
        "- 0-back control memory task:\n",
        "\n",
        "The first is a match-to-sample condition (termed in the following text as 0-Back) during which a cue “Target” image is presented at the beginning of a block and which the subject has been instructed to memorize. Then a sequence of 10 images is presented. \n",
        "\n",
        "\n",
        "Any dataset that used cognitive tests would work.\n",
        "Question: limit to behavioral data vs fMRI?\n",
        "Question: Which stimuli and actions to use?\n",
        "classic tests can be modeled using 1) bounded symbolic stimuli/actions (e.g., A, B, C), but more sophisticated one would require texts or images (e.g., face vs neutral images in social stroop dataset)\n",
        "The HCP dataset from NMA-CN contains behavioral and imaging data for 7 cognitive tests including various versions of N-back.\n",
        "\n",
        "\n",
        "Details of the tMEG Working Memory task\n",
        "\n",
        "Working memory is assessed using an N-back task in which participants are asked to monitor\n",
        "sequentially presented pictures. Participants are presented with blocks of trials that consisted of\n",
        "pictures of tools or faces. Within each run, the 2 different stimulus types are presented in\n",
        "separate blocks. Also, within each run, ½ of the blocks use a 2-back working memory task and\n",
        "½ use a 0-back working memory task (as a working memory comparison). Participants are\n",
        "instructed to press a button for every picture. If the currently presented picture matches the\n",
        "cued picture (0-Back) or the same picture that was presented two pictures before (2-Back),\n",
        "subjects press one button with their right index finger. For non-matching pictures, participants\n",
        "press a second button with their right middle finger. Two runs are performed, 16 blocks each,\n",
        "with a bright fixation \"rest\" on dark background for 15 seconds between blocks. \n",
        "\n",
        "- Special modelling of images (we are probably not going to model this)\n",
        "\n",
        "There are 2 different categories of images used in this experiment: images of faces and tools.\n",
        "Each block contains images from a single category. Some of the images in the non-matched\n",
        "trials have been characterized as “Lure”. These images have been selected so that they have\n",
        "common features with the target image, but are still different. These trials as flagged as “Lure”.\n",
        "I\n",
        "\n",
        "\n",
        "https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf Page 72"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddUxN-0M-842"
      },
      "source": [
        "---\n",
        "## Implementation scheme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment\n",
        "\n",
        "The following cell implments N-back envinronment, that we later use to train a RL agent on human data. It is capable of performing two kinds of simulation:\n",
        "- rewards the agent once the action was correct (i.e., a normative model of the environment).\n",
        "- receives human data (or mock data if you prefer), and returns what participants performed as the observation. This is more useful for preference-based RL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-62uNYF-_9j"
      },
      "outputs": [],
      "source": [
        "# @title Define environment\n",
        "\n",
        "# N-back environment\n",
        "class NBack(gym.Env):\n",
        "\n",
        "    # N = 2\n",
        "    # step_count =        [ 0  1  2  3  4  5  6 ]\n",
        "    # sequence =          [ a  b  c  d  a  d  a ]\n",
        "    # correct actions =   [ ~  ~  0  0  0  1  1 ]\n",
        "\n",
        "    # actions =           [ ~  ~  1  0  0  1  0 ]\n",
        "    # reward_class =      [ ~  ~  FP TN TN TP FN]\n",
        "    # reward =            [ ~  ~  -1  0  0  1 -1]\n",
        "\n",
        "  ACTIONS = ['match', 'non-match']\n",
        "\n",
        "  # Rewards input is structured as (TP, TN, FP, FN) (positive being matches)\n",
        "  def __init__(self, \n",
        "               N=2, \n",
        "               num_steps=10,\n",
        "               num_blocks=8, \n",
        "               num_memory = 5,\n",
        "               stimulus_choices = list(string.ascii_lowercase),\n",
        "               human_data = none):\n",
        "\n",
        "    # basic setting\n",
        "    self.N = N # N in N-back task\n",
        "    self.num_steps = num_steps # how many image showed per block\n",
        "    self.num_blocks = num_blocks # how many blocks per participant\n",
        "    self.stimulus_choices = stimulus_choices # what stimuli are available\n",
        "    self.num_stim = len(stimulus_choices)\n",
        "    self.num_memory = num_memory\n",
        "    self.step_count = 0\n",
        "\n",
        "    # Define rewards, observation space and action space\n",
        "    self.observations = np.zeros(shape = (num_stim, num_memory)).astype('int')  # will be filled in the `reset()`\n",
        "    self.reward_range = spaces.Discrete(2)                                      # Range of rewards based on inputs\n",
        "    self.action_space = spaces.Discrete(2)                                      # 0 (No match) or 1 (Match)\n",
        "    \n",
        "    self._action_history = []\n",
        "\n",
        "    # copy and pasted from template \n",
        "    # whether mimic humans or reward the agent once it responds optimally.\n",
        "    if human_data is None:\n",
        "      self._imitate_human = False\n",
        "      self.human_data = None\n",
        "      self.human_subject_data = None\n",
        "    else:\n",
        "      self._imitate_human = True\n",
        "      self.human_data = human_data\n",
        "      self.human_subject_data = None\n",
        "\n",
        "\n",
        "  def reset(self, seed=None):\n",
        "\n",
        "    # Seed RNG\n",
        "    super().reset(seed=seed)\n",
        "\n",
        "    # Generate sequence of length self.episode_length, using one hot encoding\n",
        "    observations = np.zeros(shape = (self.num_stim, self.num_memory)).astype('int')\n",
        "    for i in range(self.num_memory):\n",
        "      observations[random.randrange(self.num_stim), i] = 1\n",
        "\n",
        "    # Generate correct sequence of actions\n",
        "    correct_actions = [None] * self.N + [int(self.sequence[i] == self.sequence[i + self.N]) for i in range(self.episode_length - self.N)]\n",
        "\n",
        "    # clearing the action history and reset step count\n",
        "    self._action_history = []\n",
        "    self.step_count = 0\n",
        "\n",
        "    self.observations = observations\n",
        "    self.correct_actions = correct_actions\n",
        "\n",
        "    return observations\n",
        "\n",
        "  \n",
        "  \n",
        "  def step(self, action):\n",
        "\n",
        "    #\n",
        "    self.step_count += 1\n",
        "\n",
        "    # get reward\n",
        "    reward\n",
        "    \n",
        "    # update observations\n",
        "    \n",
        "    # update action space\n",
        "    self._action_history.append(action)\n",
        "\n",
        "\n",
        "\n",
        "    # Get reward\n",
        "    if self.step_count >= self.N:\n",
        "      if (self.correct_actions[self.step_count]): # Match\n",
        "        reward = self.rewards[0] if action else self.rewards[3] # TP if matched else FN\n",
        "      else: # No match\n",
        "        reward = self.rewards[2] if action else self.rewards[1] # FP if matches else TN\n",
        "    else:\n",
        "      reward = None\n",
        "\n",
        "    # Get next character in sequence (or end episode)\n",
        "    self.step_count += 1\n",
        "    if self.step_count < self.episode_length:\n",
        "      observation = self.sequence[self.step_count]\n",
        "      done = False\n",
        "    else:\n",
        "      observation = None\n",
        "      done = True\n",
        "    info = \"Not sure\"\n",
        "\n",
        "    return observation, reward, done, info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "buTmgeVJe78z"
      },
      "outputs": [],
      "source": [
        "# @title Test environment\n",
        "\n",
        "env = NBack()\n",
        "print(f\"First char is {env.reset()[0]}\")\n",
        "print(env.sequence)\n",
        "print(env.correct_actions)\n",
        "for i in range(16):\n",
        "  print(env.step(i % 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a random agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random agent\n",
        "class RandomAgent(nn.Module):\n",
        "    \n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "\n",
        "    # Choose a random action, 0 or 1\n",
        "    def choose_action(self, seq):\n",
        "        return np.random.randint(0, 2)\n",
        "\n",
        "    def test(self, num_episodes):\n",
        "        \n",
        "        # Arrays to hold true/false positives and false negatives\n",
        "        tps = np.zeros(num_episodes)\n",
        "        fps = np.zeros_like(tps)\n",
        "        fns = np.zeros_like(tps)\n",
        "\n",
        "        for i in range(num_episodes):\n",
        "            \n",
        "            # Array of actions\n",
        "            actions = np.zeros(self.env.episode_length, dtype=int)\n",
        "\n",
        "            # Perform action for each element in sequence\n",
        "            seq, step_count, _, done = self.env.reset()\n",
        "            while not done:\n",
        "                action = self.choose_action(seq)\n",
        "                actions[step_count] = action\n",
        "                seq, step_count, _, done = self.env.step(action)\n",
        "            \n",
        "            # Get episode data\n",
        "            actions = actions[self.env.N:]\n",
        "            correct_actions = self.env.correct_actions\n",
        "            tps[i] = np.dot(actions, correct_actions) \n",
        "            fps[i] = np.dot(actions, 1 - correct_actions)\n",
        "            fns[i] = np.dot(1 - actions, correct_actions)\n",
        "\n",
        "        return tps, fps, fns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a simple Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningMLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, env, input_size=1, hidden_sizes=[], actv=\"ReLU()\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.env = env                 \n",
        "        self.input_size = input_size\n",
        "        self.output_size = 2\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.mlp = nn.Sequential()\n",
        "\n",
        "        # Create net\n",
        "        prev_size = self.input_size # Initialize the temporary input feature to each layer\n",
        "        for i in range(len(hidden_sizes)): # Loop over layers and create each one\n",
        "            \n",
        "            # Add linear layer\n",
        "            current_size = hidden_sizes[i] # Assign the current layer hidden unit from list\n",
        "            layer = nn.Linear(prev_size, current_size)\n",
        "            prev_size = current_size # Assign next layer input using current layer output\n",
        "            self.mlp.add_module('Linear_%d'%i, layer) # Append layer to the model\n",
        "\n",
        "            # Add activation function\n",
        "            actv_layer = eval('nn.%s'%actv) # Assign activation function (eval allows us to instantiate object from string)\n",
        "            self.mlp.add_module('Activation_%d'%i, actv_layer) # Append activation to the model with a name\n",
        "\n",
        "        out_layer = nn.Linear(prev_size, self.output_size) # Create final layer\n",
        "        self.mlp.add_module('Output_Linear', out_layer) # Append the final layer\n",
        "\n",
        "        # Softmax layer\n",
        "        softmax_layer = nn.Softmax()\n",
        "        self.mlp.add_module('Output_Softmax', softmax_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Do we need to reshape?\n",
        "        input = torch.Tensor(x)\n",
        "        return self.mlp(input)\n",
        "\n",
        "    def choose_action(self, seq):\n",
        "\n",
        "        # Run sequence through network\n",
        "        output = self.forward(seq)\n",
        "        # Choose highest likelihood action\n",
        "        return np.argmax(output.detach())\n",
        "\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "\n",
        "        for i in range(num_episodes):\n",
        "\n",
        "            # Array of actions\n",
        "            actions = np.zeros(self.env.episode_length, dtype=int)\n",
        "\n",
        "            # Reset environment\n",
        "            seq, step_count, _, done = self.env.reset()\n",
        "\n",
        "            while not done:\n",
        "\n",
        "                # Choose action and recieve reward\n",
        "                action = self.choose_action(seq)\n",
        "                actions[step_count] = action\n",
        "                seq, step_count, reward, done = self.env.step(action)\n",
        "\n",
        "                # Use reward to update network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a Recurrent Deep Q-learning Agent (RDQN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attempt to define a RNN in pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Model definition\n",
        "\n",
        "https://medium.com/@VersuS_/coding-a-recurrent-neural-network-rnn-from-scratch-using-pytorch-a6c9fc8ed4a7\n",
        "\n",
        "https://www.youtube.com/watch?v=WEV61GmmPrk\n",
        "\"\"\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic RNN block. This represents a single layer of RNN\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
        "        \"\"\"\n",
        "        input_size: Number of features of your input vector\n",
        "        hidden_size: Number of hidden neurons\n",
        "        output_size: Number of features of your output vector\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # input to hidden\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size, bias=False)\n",
        "        # input to output\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        # softmax layer\n",
        "        self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    \n",
        "    def forward(self, input_tensor, hidden_tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Returns softmax(linear_out) and tanh(i2h + i2o)\n",
        "        Inputs\n",
        "        ------\n",
        "        x: Input vector x  with shape (vocab_size, )\n",
        "        hidden_state: Hidden state matrix\n",
        "        Outputs\n",
        "        -------\n",
        "        out: Prediction vector\n",
        "        hidden_state: New hidden state matrix\n",
        "        \"\"\"\n",
        "        combined = torch.cat((input_tensor, output_tensor), 1)\n",
        "\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output, hidden\n",
        "        \n",
        "\n",
        "    def init_hidden(self, batch_size=1) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns a hidden state with specified batch size. Defaults to 1\n",
        "        \"\"\"\n",
        "        return torch.zeros(1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training RNN layer\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "learning_rate = 0.005\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr = learning_rate)\n",
        "\n",
        "def train(input_tensor, action_tensor):\n",
        "    hidden = rnn.init_hidden()\n",
        "    for i in range(input_tensor.size()[0]):\n",
        "        output, hidden = rnn(input_tensor[i], hidden)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaBV8Quh-ypJ"
      },
      "source": [
        "# Section 4: Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q7TAlHUZLjnc"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unexpected EOF while parsing (3220473413.py, line 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/var/folders/g4/tfd3sjg137dg4c41qxh6l7zw0000gn/T/ipykernel_47468/3220473413.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ],
      "source": [
        "# Random agent\n",
        "class RandomAgent(nn.Module):\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-y_Z8yu_-1v"
      },
      "outputs": [],
      "source": [
        "# RNN\n",
        "\n",
        "class LayeredRNN(nn.Module)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nma",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
