{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2ueYfnZ9867"
      },
      "source": [
        "# Investigating RNNs and RL using the N-back cognitive task\n",
        "\n",
        "**NMA 2023 Group 1 Project**\n",
        "\n",
        "__Content creators:__ Alan Astudilo, Campbell Border, Disheng, Julia Yin, Koffivi\n",
        "\n",
        "__Pod TA:__ Suryanarayanan Nagar Anthel Venkatesh\n",
        "\n",
        "__Project Mentor:__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Objective\n",
        "\n",
        "- \n",
        "\n",
        "- \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwxGrBIy-jz4"
      },
      "source": [
        "# Project Design\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb2cp2_wCSCI"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6eFAloFCZog",
        "outputId": "d2f4791b-85e9-4176-8a30-c205f952f451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.22.0 Requires-Python >=3.8; 1.22.1 Requires-Python >=3.8; 1.22.2 Requires-Python >=3.8; 1.22.3 Requires-Python >=3.8; 1.22.4 Requires-Python >=3.8; 1.23.0 Requires-Python >=3.8; 1.23.0rc1 Requires-Python >=3.8; 1.23.0rc2 Requires-Python >=3.8; 1.23.0rc3 Requires-Python >=3.8; 1.23.1 Requires-Python >=3.8; 1.23.2 Requires-Python >=3.8; 1.23.3 Requires-Python >=3.8; 1.23.4 Requires-Python >=3.8; 1.23.5 Requires-Python >=3.8; 1.24.0 Requires-Python >=3.8; 1.24.0rc1 Requires-Python >=3.8; 1.24.0rc2 Requires-Python >=3.8; 1.24.1 Requires-Python >=3.8; 1.24.2 Requires-Python >=3.8; 1.24.3 Requires-Python >=3.8; 1.24.4 Requires-Python >=3.8; 1.25.0 Requires-Python >=3.9; 1.25.0rc1 Requires-Python >=3.9; 1.25.1 Requires-Python >=3.9\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.23.3 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.23.3\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "hdmf 2.5.8 requires numpy<1.21,>=1.16, but you have numpy 1.21.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!pip install jedi --quiet\n",
        "!pip install --upgrade pip setuptools wheel --quiet\n",
        "!pip install numpy==1.23.3 --quiet --ignore-installed\n",
        "!pip install gymnasium --quiet\n",
        "\n",
        "\n",
        "#!pip install dm-acme[jax] --quiet\n",
        "#!pip install dm-sonnet --quiet\n",
        "#!pip install trfl --quiet\n",
        "#!pip uninstall seaborn -y --quiet\n",
        "#!pip install seaborn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEza9W8KDK1y"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "#import time\n",
        "import numpy as np\n",
        "#import pandas as pd\n",
        "#import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figure settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Zb0JSXLiECA6"
      },
      "outputs": [],
      "source": [
        "# @title Figure settings\n",
        "from IPython.display import clear_output, display, HTML\n",
        "%matplotlib inline\n",
        "# sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Background\n",
        "\n",
        "## Replace with our own literature review\n",
        "\n",
        "- Cognitive scientists use standard lab tests to tap into specific processes in the brain and behavior. Some examples of those tests are Stroop, N-back, Digit Span, TMT (Trail making tests), and WCST (Wisconsin Card Sorting Tests).\n",
        "\n",
        "- Despite an extensive body of research that explains human performance using descriptive what-models, we still need a more sophisticated approach to gain a better understanding of the underlying processes (i.e., a how-model).\n",
        "\n",
        "- Interestingly, many of such tests can be thought of as a continuous stream of stimuli and corresponding actions, that is in consonant with the RL formulation. In fact, RL itself is in part motivated by how the brain enables goal-directed behaviors using reward systems, making it a good choice to explain human performance.\n",
        "\n",
        "- One behavioral test example would be the N-back task.\n",
        "\n",
        "  - In the N-back, participants view a sequence of stimuli, one by one, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedback is given at both timestep and trajectory levels.\n",
        "\n",
        "  - The agent is rewarded when its response matches the stimulus that was shown N steps back in the episode. A simpler version of the N-back uses two-choice action schema, that is match vs non-match. Once the present stimulus matches the one presented N step back, then the agent is expected to respond to it as being a `match`.\n",
        "\n",
        "\n",
        "- Given a trained RL agent, we then find correlates of its fitted parameters with the brain mechanisms. The most straightforward composition could be the correlation of model parameters with the brain activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets\n",
        "\n",
        "- HCP WM task ([NMA-CN HCP notebooks](https://github.com/NeuromatchAcademy/course-content/tree/master/projects/fMRI))\n",
        "\n",
        "Any dataset that used cognitive tests would work.\n",
        "Question: limit to behavioral data vs fMRI?\n",
        "Question: Which stimuli and actions to use?\n",
        "classic tests can be modeled using 1) bounded symbolic stimuli/actions (e.g., A, B, C), but more sophisticated one would require texts or images (e.g., face vs neutral images in social stroop dataset)\n",
        "The HCP dataset from NMA-CN contains behavioral and imaging data for 7 cognitive tests including various versions of N-back.\n",
        "\n",
        "### We need to copy the data and see what's happening"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## N-back task\n",
        "\n",
        "In the N-back task, participants view a sequence of stimuli, one per time, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedbacks are given at both timestep and trajectory levels.\n",
        "\n",
        "In a typical neuro setup, both accuracy and response time are measured, but here, for the sake of brevity, we focus only on accuracy of responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddUxN-0M-842"
      },
      "source": [
        "---\n",
        "## Implementation scheme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment\n",
        "### I am just copying what's on the template to here\n",
        "The following cell implments N-back envinronment, that we later use to train a RL agent on human data. It is capable of performing two kinds of simulation:\n",
        "- rewards the agent once the action was correct (i.e., a normative model of the environment).\n",
        "- receives human data (or mock data if you prefer), and returns what participants performed as the observation. This is more useful for preference-based RL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-62uNYF-_9j"
      },
      "outputs": [],
      "source": [
        "# @title Define environment\n",
        "\n",
        "# N-back environment\n",
        "class NBack(gym.Env):\n",
        "\n",
        "    # N = 2\n",
        "    # step_count =        [ 0  1  2  3  4  5  6 ]\n",
        "    # sequence =          [ a  b  c  d  a  d  a ]\n",
        "    # correct actions =   [ ~  ~  0  0  0  1  1 ]\n",
        "\n",
        "    # actions =           [ ~  ~  1  0  0  1  0 ]\n",
        "    # reward_class =      [ ~  ~  FP TN TN TP FN]\n",
        "    # reward =            [ ~  ~  -1  0  0  1 -1]\n",
        "\n",
        "  # Rewards input is structured as (TP, TN, FP, FN) (positive being matches)\n",
        "  def __init__(self, N=2, episode_length=16, chars=\"digits\", rewards=(1, 0, -1, -1)):\n",
        "\n",
        "    self.N = N\n",
        "    self.episode_length = episode_length\n",
        "    self.chars = chars\n",
        "    self.rewards = rewards\n",
        "\n",
        "    # Check that parameters are legal\n",
        "    assert(chars==\"digits\" or chars==\"letters\")\n",
        "    #assert(episode_length >= 2 and episode_length <= 32)\n",
        "\n",
        "    # Define rewards, observation space and action space\n",
        "    self.reward_range = (min(rewards), max(rewards))                                            # Range of rewards based on inputs\n",
        "    self.observation_space = spaces.Discrete(10) if chars == \"digits\" else spaces.Discrete(26)  # Single variable with 10 possibilities if using digits or 26 if using letters\n",
        "    self.action_space = spaces.Discrete(2)                                                      # 0 (No match) or 1 (Match)\n",
        "\n",
        "  def step(self, action):\n",
        "\n",
        "    # Get reward\n",
        "    if self.step_count >= self.N:\n",
        "      if (self.correct_actions[self.step_count]): # Match\n",
        "        reward = self.rewards[0] if action else self.rewards[3] # TP if matched else FN\n",
        "      else: # No match\n",
        "        reward = self.rewards[2] if action else self.rewards[1] # FP if matches else TN\n",
        "    else:\n",
        "      reward = None\n",
        "\n",
        "    # Get next character in sequence (or end episode)\n",
        "    self.step_count += 1\n",
        "    if self.step_count < self.episode_length:\n",
        "      observation = self.sequence[self.step_count]\n",
        "      done = False\n",
        "    else:\n",
        "      observation = None\n",
        "      done = True\n",
        "    info = \"Not sure\"\n",
        "\n",
        "    return observation, reward, done, info\n",
        "\n",
        "  def reset(self, seed=None):\n",
        "\n",
        "    # Seed RNG\n",
        "    super().reset(seed=seed)\n",
        "\n",
        "    # Generate sequence of length self.episode_length\n",
        "    if self.chars == \"digits\":\n",
        "      self.sequence = np.random.randint(0, 9, size=(self.episode_length))\n",
        "    else: \n",
        "      self.sequence = np.random.\n",
        "\n",
        "    # Generate correct sequence of actions\n",
        "    self.correct_actions = [None] * self.N + [int(self.sequence[i] == self.sequence[i + self.N]) for i in range(self.episode_length - self.N)]\n",
        "\n",
        "    # Observation is first character\n",
        "    self.step_count = 0\n",
        "    observation = self.sequence[self.step_count]\n",
        "    info = \"Not sure\"\n",
        "\n",
        "    return observation, info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "buTmgeVJe78z"
      },
      "outputs": [],
      "source": [
        "# @title Test environment\n",
        "\n",
        "env = NBack()\n",
        "print(f\"First char is {env.reset()[0]}\")\n",
        "print(env.sequence)\n",
        "print(env.correct_actions)\n",
        "for i in range(16):\n",
        "  print(env.step(i % 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a random agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## please write your code for random agents here\n",
        "\n",
        "# Random agent\n",
        "class RandomAgent(nn.Module):\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a simple Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a Recurrent Deep Q-learning Agent (RDQN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaBV8Quh-ypJ"
      },
      "source": [
        "# Section 4: Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q7TAlHUZLjnc"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unexpected EOF while parsing (3220473413.py, line 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/var/folders/g4/tfd3sjg137dg4c41qxh6l7zw0000gn/T/ipykernel_47468/3220473413.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ],
      "source": [
        "# Random agent\n",
        "class RandomAgent(nn.Module):\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-y_Z8yu_-1v"
      },
      "outputs": [],
      "source": [
        "# RNN\n",
        "\n",
        "class LayeredRNN(nn.Module)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "allen",
      "language": "python",
      "name": "allen"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
