{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2ueYfnZ9867"
      },
      "source": [
        "# Investigating RNNs and RL using the N-back cognitive task\n",
        "\n",
        "**NMA 2023 Group 1 Project**\n",
        "\n",
        "__Content creators:__ Alan Astudilo, Campbell Border, Disheng, Julia Yin, Koffivi\n",
        "\n",
        "__Pod TA:__ Suryanarayanan Nagar Anthel Venkatesh\n",
        "\n",
        "__Project Mentor:__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Objective\n",
        "\n",
        "- \n",
        "\n",
        "- \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwxGrBIy-jz4"
      },
      "source": [
        "# Project Design\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb2cp2_wCSCI"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6eFAloFCZog",
        "outputId": "d2f4791b-85e9-4176-8a30-c205f952f451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "%pip install jedi --quiet\n",
        "%pip install --upgrade pip setuptools wheel --quiet\n",
        "%pip install numpy==1.23.3 --quiet --ignore-installed\n",
        "%pip install gymnasium --quiet\n",
        "%pip install torch --quiet\n",
        "\n",
        "%pip install matplotlib --quiet\n",
        "%pip uninstall seaborn -y --quiet\n",
        "%pip install seaborn --quiet\n",
        "\n",
        "#!pip install trfl --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pEza9W8KDK1y"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figure settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "Zb0JSXLiECA6"
      },
      "outputs": [],
      "source": [
        "# @title Figure settings\n",
        "from IPython.display import clear_output, display, HTML\n",
        "%matplotlib inline\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Background\n",
        "\n",
        "## Replace with our own literature review\n",
        "\n",
        "- Cognitive scientists use standard lab tests to tap into specific processes in the brain and behavior. Some examples of those tests are Stroop, N-back, Digit Span, TMT (Trail making tests), and WCST (Wisconsin Card Sorting Tests).\n",
        "\n",
        "- Despite an extensive body of research that explains human performance using descriptive what-models, we still need a more sophisticated approach to gain a better understanding of the underlying processes (i.e., a how-model).\n",
        "\n",
        "- Interestingly, many of such tests can be thought of as a continuous stream of stimuli and corresponding actions, that is in consonant with the RL formulation. In fact, RL itself is in part motivated by how the brain enables goal-directed behaviors using reward systems, making it a good choice to explain human performance.\n",
        "\n",
        "- One behavioral test example would be the N-back task.\n",
        "\n",
        "  - In the N-back, participants view a sequence of stimuli, one by one, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedback is given at both timestep and trajectory levels.\n",
        "\n",
        "  - The agent is rewarded when its response matches the stimulus that was shown N steps back in the episode. A simpler version of the N-back uses two-choice action schema, that is match vs non-match. Once the present stimulus matches the one presented N step back, then the agent is expected to respond to it as being a `match`.\n",
        "\n",
        "\n",
        "- Given a trained RL agent, we then find correlates of its fitted parameters with the brain mechanisms. The most straightforward composition could be the correlation of model parameters with the brain activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets\n",
        "\n",
        "- HCP WM task ([NMA-CN HCP notebooks](https://github.com/NeuromatchAcademy/course-content/tree/master/projects/fMRI))\n",
        "\n",
        "Any dataset that used cognitive tests would work.\n",
        "Question: limit to behavioral data vs fMRI?\n",
        "Question: Which stimuli and actions to use?\n",
        "classic tests can be modeled using 1) bounded symbolic stimuli/actions (e.g., A, B, C), but more sophisticated one would require texts or images (e.g., face vs neutral images in social stroop dataset)\n",
        "The HCP dataset from NMA-CN contains behavioral and imaging data for 7 cognitive tests including various versions of N-back.\n",
        "\n",
        "### We need to copy the data and see what's happening"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## N-back task\n",
        "\n",
        "In the N-back task, participants view a sequence of stimuli, one per time, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedbacks are given at both timestep and trajectory levels.\n",
        "\n",
        "In a typical neuro setup, both accuracy and response time are measured, but here, for the sake of brevity, we focus only on accuracy of responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddUxN-0M-842"
      },
      "source": [
        "---\n",
        "## Implementation scheme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment\n",
        "The following cell implments N-back envinronment, that we later use to train a RL agent on human data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "0-62uNYF-_9j"
      },
      "outputs": [],
      "source": [
        "# N-back environment\n",
        "class NBack(gym.Env):\n",
        "\n",
        "    # Examples\n",
        "    # N = 2\n",
        "    # step_count =        [ 0  1  2  3  4  5  6 ]\n",
        "    # sequence =          [ a  b  c  d  a  d  a ] (except these are usually digits between 0-9)\n",
        "    # correct actions =   [ ~  ~  0  0  0  1  1 ]\n",
        "\n",
        "    # actions =           [ ~  ~   1     0      0  1  0 ]\n",
        "                      #           80 20 30 70 \n",
        "    # reward_class =      [ ~  ~  FP TN TN TP FN]\n",
        "    # reward =            [ ~  ~  -1  0  0  1 -1]\n",
        "\n",
        "  # Rewards input is structured as (TP, TN, FP, FN) (positive being matches)\n",
        "  def __init__(self, N=2, num_trials=25, num_targets=None, rewards=(1, 1, -1, -1), obs_length=1, seed=2023):\n",
        "\n",
        "    self.N = N\n",
        "    self.num_trials = num_trials\n",
        "    self.episode_length = num_trials + self.N\n",
        "    self.num_targets = num_targets\n",
        "    self.rewards = rewards\n",
        "    self.obs_length = obs_length\n",
        "    super().reset(seed=seed)\n",
        "\n",
        "    # Check that parameters are legal\n",
        "    assert(len(rewards) == 4)\n",
        "    assert(num_targets is None or num_targets <= num_trials)\n",
        "\n",
        "    # Define rewards, observation space and action space \n",
        "    self.reward_range = (min(rewards), max(rewards))  # Range of rewards based on inputs\n",
        "    self.observation_space = spaces.Discrete(10)      # Single variable with 10 possibilities if using digits or 26 if using letters\n",
        "    self.action_space = spaces.Discrete(2)            # 0 (No match) or 1 (Match)\n",
        "\n",
        "  def reset(self, seed=None):\n",
        "\n",
        "    # Seed RNG\n",
        "    super().reset(seed=seed)\n",
        "\n",
        "    # Generate sequence and correct actions\n",
        "    self._generate_sequence()\n",
        "    self._get_correct_actions()\n",
        "\n",
        "    # Observation is first character\n",
        "    self.step_count = 0\n",
        "    observation = self._get_observation()\n",
        "    \n",
        "\n",
        "    return observation, self.step_count, None, False\n",
        "\n",
        "  def step(self, action):\n",
        "\n",
        "    # Calculate reward\n",
        "    if self.step_count >= self.N:\n",
        "      if (self.correct_actions[self.step_count - self.N]): # Match\n",
        "        reward = self.rewards[0] if action else self.rewards[3] # TP if matched else FN\n",
        "      else: # No match\n",
        "        reward = self.rewards[2] if action else self.rewards[1] # FP if matches else TN\n",
        "    else:\n",
        "      reward = None\n",
        "\n",
        "    # Return next character or None\n",
        "    self.step_count += 1\n",
        "    if self.step_count < self.episode_length:\n",
        "      return self._get_observation(), self.step_count, reward, False\n",
        "    else:\n",
        "      return None, self.step_count, reward, True\n",
        "\n",
        "  def _generate_sequence(self):\n",
        "\n",
        "    # Generate sequence of length self.episode_length (with correct number of targets)\n",
        "    while True:\n",
        "\n",
        "      self.sequence = np.random.randint(0, 9, size=(self.episode_length))\n",
        "      if not self.num_targets or sum(self._get_correct_actions()) == self.num_targets:\n",
        "        break\n",
        "\n",
        "  def _get_observation(self):\n",
        "\n",
        "    if self.step_count < self.obs_length:\n",
        "      observation = np.concatenate((np.zeros(self.obs_length - self.step_count - 1, dtype=int), self.sequence[:self.step_count + 1]))\n",
        "    else:\n",
        "      observation = self.sequence[self.step_count + 1 - self.obs_length : self.step_count + 1]\n",
        "\n",
        "    return observation\n",
        "      \n",
        "  def _get_correct_actions(self):\n",
        "    self.correct_actions = np.array([int(self.sequence[i] == self.sequence[i + self.N]) for i in range(self.num_trials)])\n",
        "    return self.correct_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test environment\n",
        "\n",
        "env = NBack(N=2, num_trials=10, obs_length=1)\n",
        "obs, _, _, done = env.reset()\n",
        "print(env.sequence)\n",
        "print(env.step_count, obs)\n",
        "while not done:\n",
        "  obs, _, _, done = env.step(0)\n",
        "\n",
        "  print(env.step_count, obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a random agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random agent\n",
        "class RandomAgent(nn.Module):\n",
        "    \n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "\n",
        "    # Choose a random action, 0 or 1\n",
        "    def choose_action(self, seq):\n",
        "        return np.random.randint(0, 2)\n",
        "\n",
        "    def test(self, num_episodes):\n",
        "        \n",
        "        # Arrays to hold true/false positives and false negatives\n",
        "        tps = np.zeros(num_episodes)\n",
        "        fps = np.zeros_like(tps)\n",
        "        fns = np.zeros_like(tps)\n",
        "\n",
        "        for i in range(num_episodes):\n",
        "            \n",
        "            # Array of actions\n",
        "            actions = np.zeros(self.env.episode_length, dtype=int)\n",
        "\n",
        "            # Perform action for each element in sequence\n",
        "            seq, step_count, _, done = self.env.reset()\n",
        "            while not done:\n",
        "                action = self.choose_action(seq)\n",
        "                actions[step_count] = action\n",
        "                seq, step_count, _, done = self.env.step(action)\n",
        "            \n",
        "            # Get episode data\n",
        "            actions = actions[self.env.N:]\n",
        "            correct_actions = self.env.correct_actions\n",
        "            tps[i] = np.dot(actions, correct_actions) \n",
        "            fps[i] = np.dot(actions, 1 - correct_actions)\n",
        "            fns[i] = np.dot(1 - actions, correct_actions)\n",
        "\n",
        "        return tps, fps, fns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Test random agent\n",
        "# Create environment, random agent and test\n",
        "N = 2\n",
        "n = 25\n",
        "T = 5\n",
        "env = NBack(N=2, num_trials=n, num_targets=T)\n",
        "agent = RandomAgent(env=env)\n",
        "results = agent.test(10000)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "fig.suptitle(f\"Random agent with N={N}, n={n}, T={T}\")\n",
        "sns.histplot(results[0], stat=\"density\", bins=4, discrete=True)\n",
        "plt.axvline(T/2, color='r', linestyle='--')\n",
        "plt.xlabel('True positives')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.histplot(results[1], stat=\"density\", bins=17, discrete=True)\n",
        "plt.axvline((n-T)/2, color='r', linestyle='--')\n",
        "plt.xlabel('False positives')\n",
        "plt.ylabel('')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.histplot(results[2], stat=\"density\", bins=4, discrete=True)\n",
        "plt.axvline(T/2, color='r', linestyle='--')\n",
        "plt.xlabel('False negatives')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a simple Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningMLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, env, input_size=1, hidden_sizes=[], actv=\"ReLU()\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.env = env                 \n",
        "        self.input_size = input_size\n",
        "        self.output_size = 2\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.mlp = nn.Sequential()\n",
        "\n",
        "        # Create net\n",
        "        prev_size = self.input_size # Initialize the temporary input feature to each layer\n",
        "        for i in range(len(hidden_sizes)): # Loop over layers and create each one\n",
        "            \n",
        "            # Add linear layer\n",
        "            current_size = hidden_sizes[i] # Assign the current layer hidden unit from list\n",
        "            layer = nn.Linear(prev_size, current_size)\n",
        "            prev_size = current_size # Assign next layer input using current layer output\n",
        "            self.mlp.add_module('Linear_%d'%i, layer) # Append layer to the model\n",
        "\n",
        "            # Add activation function\n",
        "            actv_layer = eval('nn.%s'%actv) # Assign activation function (eval allows us to instantiate object from string)\n",
        "            self.mlp.add_module('Activation_%d'%i, actv_layer) # Append activation to the model with a name\n",
        "\n",
        "        out_layer = nn.Linear(prev_size, self.output_size) # Create final layer\n",
        "        self.mlp.add_module('Output_Linear', out_layer) # Append the final layer\n",
        "\n",
        "        # Softmax layer\n",
        "        softmax_layer = nn.Softmax()\n",
        "        self.mlp.add_module('Output_Softmax', softmax_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Do we need to reshape?\n",
        "        input = torch.Tensor(x)\n",
        "        return self.mlp(input)\n",
        "\n",
        "    def choose_action(self, seq):\n",
        "\n",
        "        # Run sequence through network\n",
        "        output = self.forward(seq)\n",
        "        # Choose highest likelihood action\n",
        "        return np.argmax(output.detach())\n",
        "\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "\n",
        "        for i in range(num_episodes):\n",
        "\n",
        "            # Array of actions\n",
        "            actions = np.zeros(self.env.episode_length, dtype=int)\n",
        "\n",
        "            # Reset environment\n",
        "            seq, step_count, _, done = self.env.reset()\n",
        "\n",
        "            while not done:\n",
        "\n",
        "                # Choose action and recieve reward\n",
        "                action = self.choose_action(seq)\n",
        "                actions[step_count] = action\n",
        "                seq, step_count, reward, done = self.env.step(action)\n",
        "\n",
        "                # Use reward to update network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test QLearningMLP\n",
        "N = 2\n",
        "num_trials = 25\n",
        "memory = 4\n",
        "\n",
        "env = NBack(N=N, num_trials=num_trials, obs_length=memory)\n",
        "agent = QLearningMLP(env=env, input_size=memory, hidden_sizes=[64])\n",
        "\n",
        "agent.train(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a Recurrent Deep Q-learning Agent (RDQN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
